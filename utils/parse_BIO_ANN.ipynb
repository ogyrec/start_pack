{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from Preprocess.SentTokenizer import SentTokenizer\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import tensorflow as tf\n",
    "from estimators import ElmoModel\n",
    "import json\n",
    "from brat_parser import get_entities_relations_attributes_groups"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# converter BIO -> ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_d = {\n",
    "    'B_ORG':'Company',\n",
    "    'B_PER':'Person',\n",
    "    'B_MEDIA':'Media',\n",
    "    'B_SPORT':'Sport',\n",
    "    'B_LOC':'Location'\n",
    "}\n",
    "\n",
    "\n",
    "def get_span(text_splt, id_):\n",
    "    lens = 0\n",
    "    ids_ = id_\n",
    "    while ids_ >= 0:\n",
    "        lens += len(text_splt[ids_])\n",
    "        ids_ -= 1\n",
    "    lens += id_\n",
    "    return lens-len(text_splt[id_]), lens\n",
    "\n",
    "def from_BIO_to_ANN(bio, text, text_splt):\n",
    "\n",
    "    d_ = defaultdict(dict)\n",
    "    ann_d = defaultdict(dict)\n",
    "    \n",
    "    for id_ , tag in enumerate(bio):\n",
    "        span_start = None\n",
    "        d_[id_] = {\n",
    "                    'word':text_splt[id_], \n",
    "                    'span_start': get_span(text_splt, id_)[0],\n",
    "                    'span_end': get_span(text_splt, id_)[1],\n",
    "                    'tag':tag\n",
    "                  }\n",
    "\n",
    "    for k, v in d_.items():\n",
    "        if v['tag'].startswith('B_'):\n",
    "            how_to_go_back = 0\n",
    "            ann_T = 'T'+str(k)\n",
    "            ann_d[ann_T] = {\n",
    "                'type': type_d[v['tag']],\n",
    "                'span_start': v['span_start'],\n",
    "                'span_end': v['span_end'],\n",
    "            }\n",
    "        if v['tag'].startswith('I_'):\n",
    "            how_to_go_back += 1\n",
    "            ann_T = 'T'+str(k-how_to_go_back)\n",
    "            ann_d[ann_T]['span_end'] = v['span_end']\n",
    "\n",
    "    for k in ann_d.keys():\n",
    "        ann_d[k]['words'] = text[ann_d[k]['span_start']:ann_d[k]['span_end']]\n",
    "        \n",
    "    return ann_d\n",
    "\n",
    "def ann_to_string(ann_d):\n",
    "    # {'T6': {'type': 'Media',\n",
    "    #               'span_start': 33,\n",
    "    #               'span_end': 51,\n",
    "    #               'words': '\" Время новостей \"'},\n",
    "    #   ->\n",
    "    # T6 Media 33 51 \" Время новостей \"\n",
    "    \n",
    "    string = ''\n",
    "    for k , v in ann_d.items():\n",
    "        string += k + '\t'\n",
    "        string += v['type'] + ' '\n",
    "        string += str(v['span_start']) + ' '\n",
    "        string += str(v['span_end']) + '\t'\n",
    "        string += v['words'] + '\\n'\n",
    "    return string    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ожидается, что на Итурупе Медведев посетит ряд социальных и инфраструктурных объектов.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['O', 'O', 'O', 'O', 'B_LOC', 'B_PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snts_error = sentences[542]\n",
    "print(snts_error)\n",
    "model.predict_fast([snts_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {'T4': {'type': 'Location',\n",
       "              'span_start': 19,\n",
       "              'span_end': 26,\n",
       "              'words': 'Итурупе'},\n",
       "             'T5': {'type': 'Person',\n",
       "              'span_start': 27,\n",
       "              'span_end': 35,\n",
       "              'words': 'Медведев'}})"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bio_str = 'O O O O O O B_MEDIA I_MEDIA I_MEDIA I_MEDIA O B_SPORT B_LOC B_PER I_PER O'\n",
    "bio = ['O', 'O', 'O', 'O', 'B_LOC', 'B_PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "text = 'Ожидается , что на Итурупе Медведев посетит ряд социальных и инфраструктурных объектов .'\n",
    "text_splt = text.split()\n",
    "\n",
    "\n",
    "ann_d = from_BIO_to_ANN(bio, text, text_splt)\n",
    "string_wrt = ann_to_string(ann_d)\n",
    "with open(\"made_ANN_TXT/0.ann\", \"w\") as text_file:\n",
    "    text_file.write(string_wrt)\n",
    "with open(\"made_ANN_TXT/0.txt\", \"w\") as text_file:\n",
    "    text_file.write(text)    \n",
    "ann_d    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### конвертор ANN -> BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_ann = {\n",
    "    'Company':'ORG',\n",
    "    'Person':'PER',\n",
    "    'Media':'MEDIA',\n",
    "    'Sport':'SPORT',\n",
    "    'Location':'LOC'\n",
    "}\n",
    "\n",
    "def from_ANN_to_BIO(entities, lines):\n",
    "\n",
    "    txt = lines[0]\n",
    "    ann_dict = defaultdict(dict)\n",
    "    position_ann_dict = defaultdict(dict)\n",
    "\n",
    "    for k, v in entities.items():\n",
    "        ann_dict[v.span[0]] = {\n",
    "            'type':v.type,\n",
    "            'text':v.text,\n",
    "            'T': k\n",
    "        }\n",
    "\n",
    "\n",
    "    for k, v in ann_dict.items():\n",
    "        start = k[0]\n",
    "        aa = v['text'].split()\n",
    "        for a in aa:\n",
    "            position_a = start + len(a)\n",
    "            position_ann_dict[(start,position_a)]={\n",
    "                'text': txt[start:position_a],\n",
    "                'type': v['type'],\n",
    "                'T':v['T']\n",
    "            }\n",
    "            start = position_a + 1\n",
    "\n",
    "\n",
    "    start = 0\n",
    "    end = 0\n",
    "    brat_bio_list = []\n",
    "    last_T = None\n",
    "\n",
    "    \n",
    "    for pst, v in enumerate(txt.split()):\n",
    "        if pst > 0:\n",
    "            end += 1\n",
    "        end += len(v)\n",
    " \n",
    "        if position_ann_dict[(start, end)]:\n",
    "            T = position_ann_dict[(start, end)]['T']\n",
    "            if last_T is not None and last_T == T:\n",
    "                brat_bio_list.append('I_' + type_ann[position_ann_dict[(start, end)]['type']])\n",
    "            else:\n",
    "                brat_bio_list.append('B_' + type_ann[position_ann_dict[(start, end)]['type']])\n",
    "                last_T = position_ann_dict[(start, end)]['T']\n",
    "            last_T = position_ann_dict[(start, end)]['T']                \n",
    "        else:\n",
    "            brat_bio_list.append('O')\n",
    "            last_T = None\n",
    "        start = end  + 1\n",
    "\n",
    "    return brat_bio_list        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "file_name = '2'\n",
    "entities, _, _, _ = get_entities_relations_attributes_groups(f\"made_ANN_TXT/{file_name}.ann\")\n",
    "with open(f\"made_ANN_TXT/{file_name}.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "from_ANN_to_BIO(entities, lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test bio -> ann -> bio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bio_ann_bio(bio_1, bio_2, text_splt):\n",
    "    for b1, b2 in zip(bio_1, bio_2):\n",
    "        if b1 != b2:\n",
    "            # return f\"b1 {b1} \\n b2 {b2} \\n bio_1 {bio_1}\\n bio_2 {bio_2}\\n text_splt {text_splt}\\n\"\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "model = ElmoModel(sess)\n",
    "head_dir = \"./NER_MODEL/\"\n",
    "model.restore(head_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with open(path) as f:\n",
    "        sentences = json.load(f)\n",
    "    return sentences\n",
    "\n",
    "MIN_LEN = 30\n",
    "MAX_LEN = 400\n",
    "\n",
    "sentences = load_data('data_1m_sentences.json')\n",
    "print(f'было {len(sentences)}')\n",
    "sentences = [x for x in sentences if MIN_LEN < len(str(x)) < MAX_LEN]\n",
    "print(f'стало {len(sentences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_fast(snts)\n",
    "for e, p in enumerate(preds):\n",
    "    if e not in bad_snts:\n",
    "        try:        \n",
    "            text_splt = model._tokenize(snts[e])\n",
    "            text_after_tknz = ' '.join(text_splt)\n",
    "            ann_d = from_BIO_to_ANN(p, text_after_tknz, text_splt)\n",
    "            string_wrt = ann_to_string(ann_d)\n",
    "\n",
    "            with open(f\"made_ANN_TXT/{e}.ann\", \"w\") as text_file:\n",
    "                text_file.write(string_wrt)\n",
    "\n",
    "            with open(f\"made_ANN_TXT/{e}.txt\", \"w\") as text_file:\n",
    "                text_file.write(text_after_tknz)        \n",
    "\n",
    "            with open(f\"made_ANN_TXT/{e}.txt\", \"r\") as f:\n",
    "                lines = f.readlines()  \n",
    "\n",
    "            entities, _, _, _ = get_entities_relations_attributes_groups(f\"made_ANN_TXT/{e}.ann\")\n",
    "            bio_1 = from_ANN_to_BIO(entities, lines)  \n",
    "            assert len(bio_1) == len(p), \"Размер листов не совпадает\"\n",
    "            assert test_bio_ann_bio(bio_1, p, text_splt), \"Ошибка в создании BIO->ANN->BIO\"\n",
    "        except Exception as ex:\n",
    "            snts_error = snts[e]\n",
    "            print(f'#{e}\\n')\n",
    "            print(f'Exception {ex}\\n')\n",
    "            print(f'snts[e] {snts_error}\\n')                \n",
    "            print(f'string_wrt {string_wrt}\\n')\n",
    "            print(f'bio_1 {bio_1}\\n')        \n",
    "            print(f'p {p}\\n')             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
